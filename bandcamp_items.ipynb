{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "# from distributed import Client\n",
    "# client = Client()\n",
    "import pandas as pd\n",
    "# import modin.pandas as pd\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from urllib.request import urlretrieve\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "from google.cloud import bigquery\n",
    "from prefect import task, flow\n",
    "from prefect_gcp.cloud_storage import GcsBucket\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq 1-Define a function to convert the downloaded file to data frame\n",
    "def read_df(file: str) -> pd.DataFrame:\n",
    "    with open(file) as data_file:\n",
    "        data = json.load(data_file)\n",
    "        df = pd.read_json(data)\n",
    "        df = pd.json_normalize(df.to_dict(\"records\"), sep=\"_\")\n",
    "        return df\n",
    "    \n",
    "# file = \"/Users/reneboygarcia/Library/CloudStorage/GoogleDrive-reneboygarcia@gmail.com/My Drive/Personal/Data Science Notebook/Data Engineering-Zoomcamp/week_7_capstone_project/albums-json/albums-full-info-10.json\"\n",
    "# df = read_df(file)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq 2-Define a function to tweak the data frame\n",
    "def tweak_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(f\"Number of rows: {df.shape[0]}\")\n",
    "    df_ = df\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq 3-Define a function to set a path for GCS storage and for local file\n",
    "def write_local(df: pd.DataFrame, filename: str) -> Path:\n",
    "    directory = Path(\"bandcamp\")\n",
    "    _file_name = filename.split(\".\")[0]\n",
    "    path_name = directory / f\"{_file_name}.parquet\"\n",
    "    try:\n",
    "        os.makedirs(directory)\n",
    "        df.to_parquet(path_name, compression=\"snappy\", index=False)\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "    return path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq 4-Define a function to upload local file to GCS Bucket\n",
    "def write_to_gcs(path: Path) -> None:\n",
    "    gcs_block = GcsBucket.load(\"prefect-gcs-block-bandcamp\")\n",
    "    gcs_block.upload_from_path(from_path=path, to_path=path)\n",
    "    print(\"Hooray, we uploaded a huge file in GCS\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq 5-Delete local file and its directory\n",
    "def duduplicate(path: Path) -> None:\n",
    "    try:\n",
    "        path.unlink()\n",
    "        full_path = path.resolve()\n",
    "        full_path.parent.rmdir()\n",
    "        print(\"Successfully deleted directory and its files\")\n",
    "    except OSError as error:\n",
    "        print(f\"Unable to find directory: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ETL from web to gcs:\n",
    "def etl_web_to_gcs(file: str):\n",
    "    # Seq 1 -Read file\n",
    "    df = read_df(file)\n",
    "    # Seq 2 -Tweak df\n",
    "    df_ = tweak_df(df)\n",
    "    # Seq 3 -Set a path this will be use to convert file to parquet\n",
    "    path_file = write_local(df, file)\n",
    "    # Seq 4-Upload local file to GCS Bucket\n",
    "    write_to_gcs(path_file)\n",
    "    # Seq 5- Remove duplicate\n",
    "    duduplicate(path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define download progress hook\n",
    "def download_progress_hook(block_num, block_size, total_size, progress_bar=None):\n",
    "    if not progress_bar:\n",
    "        progress_bar = tqdm(total=total_size, unit=\"B\", unit_scale=True)\n",
    "    downloaded = block_num * block_size\n",
    "    progress_bar.update(downloaded - progress_bar.n)\n",
    "    if downloaded >= total_size:\n",
    "        progress_bar.close()\n",
    "    return progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq 0 -Download file folder from web\n",
    "def fetch_data(url:str):\n",
    "    folder_name = url.split(\"/\")[-1].split(\"?\")[0]\n",
    "    file_folder = urlretrieve(url, folder_name, reporthook=download_progress_hook)\n",
    "    if folder_name.endswith(\".zip\"):\n",
    "        zip_file = ZipFile(folder_name)\n",
    "        folder_name_ = os.path.commonprefix(zip_file.namelist()).strip(\"/\")\n",
    "        zip_file.extractall()\n",
    "        print(f\"Download Complete..extracted zip file\")\n",
    "        print(f\"Extracted folder path: {folder_name_}\")\n",
    "        return folder_name_\n",
    "    print(f\"Download Complete..\")\n",
    "    return file_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a parent ETL to download the files\n",
    "progress_bar = None\n",
    "def elt_parent_web_gcs():\n",
    "    # Parameters\n",
    "    dataset_url = \"https://www.dropbox.com/s/a1kl5e35j4o53mz/bandcamp-items-json.zip?dl=1\"\n",
    "\n",
    "    # Execution\n",
    "    # Seq 0 -Download file folder from web\n",
    "    file_folder = fetch_data(dataset_url)\n",
    "    # Loop through the files then run etl_web_to_gcs\n",
    "    print(\"Running etl_web_to_gcs...this will take sometime..grab some coffee or tea\")\n",
    "    for file in os.listdir(file_folder)[:1]:\n",
    "        if file.endswith(\".json\"):\n",
    "            file_path = os.path.join(file_folder, file)\n",
    "            print(f\"Running: {file}\")\n",
    "            etl_web_to_gcs(file_path)\n",
    "            print(f\"Done uploading {file} to GCS\")\n",
    "    print(\"All files are Uploaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_url = \"https://www.dropbox.com/s/a1kl5e35j4o53mz/bandcamp-items-json.zip?dl=1\"\n",
    "dataset_url_2 = \"https://www.dropbox.com/s/wd38q80el16i19q/1000000-bandcamp-sales.zip?dl=1\"\n",
    "file_folder = fetch_data(dataset_url_2)\n",
    "for file in os.listdir(file_folder):\n",
    "    if file.endswith(\".json\"):\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.listdir(file_folder))\n",
    "# print(os.listdir(\"albums-json\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    elt_parent_web_gcs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
