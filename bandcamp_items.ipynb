{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "# from distributed import Client\n",
    "# client = Client()\n",
    "import pandas as pd\n",
    "# import modin.pandas as pd\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from urllib.request import urlretrieve\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "from google.cloud import bigquery\n",
    "from prefect import task, flow\n",
    "from prefect_gcp.cloud_storage import GcsBucket\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq 1-Define a function to convert the downloaded file to data frame\n",
    "def read_df(file: str) -> pd.DataFrame:\n",
    "    with open(file) as data_file:\n",
    "        data = json.load(data_file)\n",
    "        df = pd.read_json(data)\n",
    "        df = pd.json_normalize(df.to_dict(\"records\"), sep=\"_\")\n",
    "        return df\n",
    "    \n",
    "# file = \"/Users/reneboygarcia/Library/CloudStorage/GoogleDrive-reneboygarcia@gmail.com/My Drive/Personal/Data Science Notebook/Data Engineering-Zoomcamp/week_7_capstone_project/albums-json/albums-full-info-10.json\"\n",
    "# df = read_df(file)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq 2-Define a function to tweak the data frame\n",
    "def tweak_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(f\"Number of rows: {df.shape[0]}\")\n",
    "    df_ = df\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq 3-Define a function to set a path for GCS storage and for local file\n",
    "def write_local(df: pd.DataFrame, filename: str) -> Path:\n",
    "    directory = Path(\"bandcamp\")\n",
    "    _file_name = filename.split(\".\")[0]\n",
    "    path_name = directory / f\"{_file_name}.parquet\"\n",
    "    try:\n",
    "        os.makedirs(directory)\n",
    "        df.to_parquet(path_name, compression=\"snappy\", index=False)\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "    return path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq 4-Define a function to upload local file to GCS Bucket\n",
    "def write_to_gcs(path: Path) -> None:\n",
    "    gcs_block = GcsBucket.load(\"prefect-gcs-block-bandcamp\")\n",
    "    gcs_block.upload_from_path(from_path=path, to_path=path)\n",
    "    print(\"Hooray, we uploaded a huge file in GCS\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq 5-Delete local file and its directory\n",
    "def duduplicate(path: Path) -> None:\n",
    "    try:\n",
    "        path.unlink()\n",
    "        full_path = path.resolve()\n",
    "        full_path.parent.rmdir()\n",
    "        print(\"Successfully deleted directory and its files\")\n",
    "    except OSError as error:\n",
    "        print(f\"Unable to find directory: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ETL from web to gcs:\n",
    "def etl_web_to_gcs(file: str):\n",
    "    # Seq 1 -Read file\n",
    "    df = read_df(file)\n",
    "    # Seq 2 -Tweak df\n",
    "    df_ = tweak_df(df)\n",
    "    # Seq 3 -Set a path this will be use to convert file to parquet\n",
    "    path_file = write_local(df, file)\n",
    "    # Seq 4-Upload local file to GCS Bucket\n",
    "    write_to_gcs(path_file)\n",
    "    # Seq 5- Remove duplicate\n",
    "    duduplicate(path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define download progress hook\n",
    "def download_progress_hook(block_num, block_size, total_size):\n",
    "    global progress_bar\n",
    "    if not progress_bar:\n",
    "        progress_bar = tqdm(total=total_size, unit='B', unit_scale=True)\n",
    "    downloaded = block_num * block_size\n",
    "    progress_bar.update(downloaded - progress_bar.n)\n",
    "    if downloaded >= total_size:\n",
    "        progress_bar = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq 0 -Download file folder from web\n",
    "def fetch_data(url:str):\n",
    "    folder_name = url.split(\"/\")[-1].split(\"?\")[0]\n",
    "    file_folder = urlretrieve(url, folder_name, reporthook=download_progress_hook)\n",
    "    if folder_name.endswith(\".zip\"):\n",
    "        zip_file = ZipFile(folder_name)\n",
    "        folder_name_ = os.path.commonprefix(zip_file.namelist()).strip(\"/\")\n",
    "        zip_file.extractall()\n",
    "        print(f\"Download Complete..extracted zip file\")\n",
    "        print(f\"Extracted folder path: {folder_name_}\")\n",
    "        return folder_name_\n",
    "    print(f\"Download Complete..\")\n",
    "    return file_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a parent ETL to download the files\n",
    "progress_bar = None\n",
    "def elt_parent_web_gcs():\n",
    "    # Parameters\n",
    "    dataset_url = \"https://www.dropbox.com/s/a1kl5e35j4o53mz/bandcamp-items-json.zip?dl=1\"\n",
    "\n",
    "    # Execution\n",
    "    # Seq 0 -Download file folder from web\n",
    "    file_folder = fetch_data(dataset_url)\n",
    "    # Loop through the files then run etl_web_to_gcs\n",
    "    print(\"Running etl_web_to_gcs...this will take sometime..grab some coffee or tea\")\n",
    "    for file in os.listdir(file_folder)[:1]:\n",
    "        if file.endswith(\".json\"):\n",
    "            file_path = os.path.join(file_folder, file)\n",
    "            print(f\"Running: {file}\")\n",
    "            etl_web_to_gcs(file_path)\n",
    "            print(f\"Done uploading {file} to GCS\")\n",
    "    print(\"All files are Uploaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_url = \"https://www.dropbox.com/s/a1kl5e35j4o53mz/bandcamp-items-json.zip?dl=1\"\n",
    "\n",
    "# file_folder = fetch_data(dataset_url)\n",
    "# for file in os.listdir(file_folder):\n",
    "#     if file.endswith(\".json\"):\n",
    "#         print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.listdir(file_folder))\n",
    "# print(os.listdir(\"albums-json\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "988MB [07:00, 2.35MB/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Complete..extracted zip file\n",
      "Extracted folder path: albums-json\n",
      "Running etl_web_to_gcs...this will take sometime..grab some coffee or tea\n",
      "Running: albums-full-info-3.json\n",
      "Number of rows: 100000\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'to_parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     elt_parent_web_gcs()\n",
      "Cell \u001b[0;32mIn[74], line 16\u001b[0m, in \u001b[0;36melt_parent_web_gcs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(file_folder, file)\n\u001b[1;32m     15\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRunning: \u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m         etl_web_to_gcs(file_path)\n\u001b[1;32m     17\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDone uploading \u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m to GCS\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAll files are Uploaded\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[71], line 8\u001b[0m, in \u001b[0;36metl_web_to_gcs\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      6\u001b[0m df_ \u001b[39m=\u001b[39m tweak_df(df)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Seq 3 -Set a path this will be use to convert file to parquet\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m path_file \u001b[39m=\u001b[39m write_local(df, file)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Seq 4-Upload local file to GCS Bucket\u001b[39;00m\n\u001b[1;32m     10\u001b[0m write_to_gcs(path_file)\n",
      "Cell \u001b[0;32mIn[68], line 8\u001b[0m, in \u001b[0;36mwrite_local\u001b[0;34m(df, filename)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(directory)\n\u001b[0;32m----> 8\u001b[0m     pd\u001b[39m.\u001b[39;49mto_parquet(path_name, compression\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(error)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/pandas/__init__.py:264\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrays\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m SparseArray \u001b[39mas\u001b[39;00m _SparseArray\n\u001b[1;32m    262\u001b[0m     \u001b[39mreturn\u001b[39;00m _SparseArray\n\u001b[0;32m--> 264\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpandas\u001b[39m\u001b[39m'\u001b[39m\u001b[39m has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'to_parquet'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    elt_parent_web_gcs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
